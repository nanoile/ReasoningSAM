<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>InstructSAM</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            max-width: 800px;
            margin: auto;
            color: #333;
        }
        header {
            text-align: center;
            margin-bottom: 20px;
        }
        h1 {
            color: #0366d6;
        }
        .links {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        .links a {
            text-decoration: none;
            color: white;
            font-weight: bold;
            background-color: #333;
            padding: 8px 16px;
            border-radius: 20px;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: background-color 0.3s ease;
            font-size: 14px;
        }
        .links a:hover {
            background-color: #555;
        }
        .links a.arxiv {
            background-color: #b31b1b;
        }
        .links a.arxiv:hover {
            background-color: #d42c2c;
        }
        .links a.github {
            background-color: #333;
        }
        .links a.github:hover {
            background-color: #555;
        }
        .links a.colab {
            background-color: #f9ab00;
        }
        .links a.colab:hover {
            background-color: #ffc107;
        }
        .links a.model {
            background-color: #6f42c1;
        }
        .links a.model:hover {
            background-color: #8a63d2;
        }
        .content {
            margin-top: 30px;
        }
        .badge {
            height: 20px;
            vertical-align: middle;
        }
    </style>
</head>
<body>
    <header>
        <h1>InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition</h1>
        <p></p>

    </header>
    <div align="center">
        <a href='https://voyagerxvoyagerx.github.io/' target='_blank'>Yijie Zheng</a><sup>1,2</sup>&emsp;
        <a href='https://github.com/go-bananas-wwj' target='_blank'>Weijie Wu</a><sup>1,2</sup>&emsp;
        <a href='https://scholar.google.com/citations?hl=en&user=TvsTun4AAAAJ' target='_blank'>Qingyun Li</a><sup>3</sup>&emsp;
        <a href='https://huiserwang.site/' target='_blank'>Xuehui Wang</a><sup>4</sup>&emsp;
        <a href='https://orcid.org/0009-0009-5003-0471' target='_blank'>Xu Zhou</a><sup>5</sup>&emsp;
        <br>
        <a href='https://orcid.org/0009-0007-1808-5347' target='_blank'>Aiai Ren</a><sup>5</sup>&emsp;
        <a href='https://scholars.uow.edu.au/jun-shen' target='_blank'>Jun Shen</a><sup>5</sup>&emsp;
        <a href='https://www.researchgate.net/profile/Zhao-Long-12' target='_blank'>Long Zhao</a><sup>2</sup>&emsp;
        <a href='https://scholar.google.com/citations?hl=en&user=JGeMHTAAAAAJ&view_op=list_works&sortby=pubdate' target='_blank'>Guoqing Li</a><sup>✉️2</sup>&emsp;
        <a href='https://yangxue0827.github.io/' target='_blank'>Xue Yang</a><sup>4</sup>&emsp;
    </div>
    
    <div align="center">
        <sup>1</sup>University of Chinese Academy of Sciences&emsp;
        <sup>2</sup>Aerospace Information Research Institute&emsp; <br>
        <sup>3</sup>Harbin Institute of Technology&emsp; 
        <sup>4</sup>Shanghai Jiao Tong University&emsp;
        <sup>5</sup>University of Wollongong&emsp;
    </div>
    <div class="links">
        <a href="https://voyagerxvoyagerx.github.io/InstructSAM/InstructSAM_Camera_Ready.pdf" target="_blank" class="arxiv">
            <i class="fas fa-file-alt"></i>
            PDF
        </a>
        <a href="https://github.com/VoyagerXvoyagerx/InstructSAM" target="_blank" class="github">
            <i class="fab fa-github"></i>
            Code
        </a>
        <a href="https://colab.research.google.com/drive/1Ya7h04ZRPuHv3b934VoGJRMI0QpkH2oo?usp=sharing" target="_blank" class="colab">
            <i class="fas fa-play"></i>
            Colab
        </a>
        <!-- <a href="https://colab.research.google.com/drive/1Ya7h04ZRPuHv3b934VoGJRMI0QpkH2oo?usp=sharing" target="_blank" class="github">
            <i class="fas fa-play"></i>
            <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" > -->
            
    </div>
    <div class="content">
        <h2>About InstructSAM</h2>
        <p>Instruction-based object recognition has emerged as a powerful paradigm in computer vision. However, the lack of semantically diverse training data has limited the zero-shot performance of vision-language models in remote sensing.
            InstructSAM introduces a training-free framework for Instruction-Oriented Object Counting, Detection, and Segmentation (InstructCDS) tasks across open-vocabulary, open-ended, and open-subclass settings.
            By reformulating object detection as a counting-constrained mask-label matching problem, it enables confidence-free object recognition and achieves near-constant inference time regardless of object counts.</p>
        
        <h2>InstructCDS Tasks & EarthInstruct Benchmark</h2>
        <p>The EarthInstruct benchmark introduces three challenging instruction settings:</p>
        <ul>
            <li><strong>Open-Vocabulary:</strong> Recognition with user-specified categories (e.g., "soccer field", "football field", "parking lot").</li>
            <li><strong>Open-Ended:</strong> Recognition of all visible objects without specifying categories.</li>
            <li><strong>Open-Subclass:</strong> Recognition of objects within a super-category.</li>
        </ul>

        <img src="./assets/task_settings.png" alt="Task Settings" style="width: 100%;">

        <p>
        Beyond the basic three settings, we employ dataset-specific prompts that guide LVLMs to
        recognize objects according to specific annotation rules, addressing versatile user requirements and real-world dataset biases (examples shown below).</p>
        <img src="./assets/dataset_bias.png" alt="Dataset bias" style="width: 100%;">
        
        
        <h2>InstructSAM Framework</h2>
        <img src="./assets/framework.png" alt="InstructSAM Framework" style="width: 100%;">
        <p>To tackle the challenges of limited training data and complex user instructions, InstructSAM decomposes instruction-oriented object detection into three tractable steps:</p>
        
        <p><strong>Step 1: Instruction-Oriented Object Counting</strong><br>
        A large vision-language model (LVLM) interprets user instructions and predicts <strong>object categories</strong> and <strong>counts</strong>.</p>
        
        <p><strong>Step 2: Class-Agnostic Mask Generation</strong><br>
        SAM2 automatically generates high-quality <strong>mask proposals</strong> in parallel with instruction processing.</p>
        
        <p><strong>Step 3: Counting-Constrained Matching</strong><br>
        A remote sensing CLIP model computes <strong>semantic similarity</strong> between predicted categories and mask proposals.
        InstructSAM formulates object detection and segmentation as a <strong>mask-label matching</strong> problem, integrating semantic similarity with global counting constraints.
        A binary integer programming solver is used to solve the matching problem and obtain the final recognition results.</p>
                
        <img src="./assets/inference_process.jpg" alt="InstructSAM Inference Process" style="width: 100%;">
        <div style="text-align: center;">Visualization of the InstructSAM inference process.</div> <br>

        <img src="./assets/results_main.png" alt="Results Visualization" style="width: 100%;">
        <div style="text-align: center;">Qualitative results across different settings.</div> <br>
        
        <h2>Key Results & Performance Highlights</h2>
        <ul>
            <li><strong>State-of-the-Art Performance:</strong> Matches or surpasses specialized baselines that trained on large-scale task-specific data on the EarthInstruct benchmark.</li>
            <li><strong>Training-Free & Confidence-Free:</strong> Requires no task-specific training or fine-tuning, and its matching process eliminates the need for confidence threshold filtering.</li>
            <li><strong>Efficient Inference:</strong> Achieves near-constant inference time regardless of the number of objects, significantly reducing output tokens and overall runtime compared to direct generation approaches.</li>
            <li><strong>Strong Generalization:</strong> Demonstrates generalization to natural images when equipped with generic CLIP models.</li>
        </ul>
        
        <h3>Open-Vocabulary Results</h3>
        <div style="text-align: center;"><img src="./assets/table_1.png" alt="Open-Vocabulary Results" style="width: 80%;"></div>

        <h3>Open-Ended Results</h3>
        <div style="text-align: center;"><img src="./assets/table_2.png" alt="Open-Ended Results" style="width: 65%;"></div>
        
        <h3>Open-Subclass Results</h3>
        <div style="text-align: center;"><img src="./assets/table_3.png" alt="Open-Subclass Results" style="width: 92%;"></div>

        <h3>Inference Time Analysis</h3>
        <div style="display: flex; align-items: center; gap: 20px; margin: 20px 0;">
            <div style="flex: 0 0 50%;">
                <img src="./assets/inference_time.png" alt="Inference Time Comparison" style="width: 100%;">
            </div>
            <div style="flex: 1; font-size: 16px; line-height: 1.5;">
                InstructSAM exhibits nearly constant inference speed under open-ended setting, in contrast to other approaches whose runtime increases linearly with object count. 
                Unlike methods that represent bounding boxes as natural language tokens, InstructSAM reduces output tokens by 89% and total inference time by 32% compared to Qwen2.5-VL.
                This advantage becomes more pronounced as model size scales up, highlighting the efficiency of our framework.
            </div>
        </div>

        <h3>Generalization to Natural Images</h3>
        <div style="text-align: center;"><img src="./assets/natural_image.png" alt="Natural Image Results" style="width: 70%;"></div>
        <div style="text-align: center;">When equipped with generic CLIP, InstructSAM can effectively recognize objects in natural images.</div>
        
        
        <h2>Analysis & Discussion</h2>
        <h3>The Power of Foundation Models and Prompt Engineering</h3>
        <div style="display: flex; align-items: center; gap: 20px; margin: 20px 0;">
            <div style="flex: 0 0 50%;">
                <img src="./assets/table_4.png" alt="Counting Performance Table" style="width: 100%;"> * The Faster-RCNN is trained on DIOR training set.
            </div>
            <div style="flex: 1; font-size: 16px; line-height: 1.5;">
                Providing GPT-4o with detailed annotation rules enables it to count objects as accurately as a close-set trained Faster-RCNN! This demonstrates the importance of proper prompt design in leveraging foundation model capabilities.
            </div>
        </div>
        
        <h3>Confidence-Free vs. Confidence-Based Approaches</h3>
        <div style="text-align: center;"><img src="./assets/threshold.png" alt="Threshold Sensitivity Analysis" style="width: 70%;"></div>
        <p>Traditional detectors rely on confidence scores and thresholds, which can be sensitive and difficult to tune, especially in zero-shot scenarios.
            InstructSAM's counting-constrained matching approach provides a robust alternative by dynamically adjusting assignments based on predicted counts from the LVLM.</p>

        <h3>Limitations & Future Directions</h3>
        <p>InstructSAM's performance depends on the capabilities of the underlying foundation models (LVLM, SAM2, CLIP).
            Future advancements in these models, particularly those trained on more semantically diverse remote sensing data, will likely enhance InstructSAM's capabilities further.</p>
        
        <h2>Getting Started</h2>
        <p>Ready to try InstructSAM? Check out our <a href="https://github.com/VoyagerXvoyagerx/InstructSAM/blob/main/README.md" target="_blank">README</a> for detailed installation and usage instructions.</p>
        <h2>Citation</h2>
        <pre><code>@article{zheng2025instructsam,
    title={InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition}, 
    author={Yijie Zheng and Weijie Wu and Qingyun Li and Xuehui Wang and Xu Zhou and Aiai Ren and Jun Shen and Long Zhao and Guoqing Li and Xue Yang},
    year={2025},
    journal={arXiv preprint arXiv:2505.15818},
}</code></pre>
    </div>
</body>
</html> 
